{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Close Crawl.ipynb","provenance":[],"collapsed_sections":["tkrJRvsCwL4E","uchYLV629hp6","-yoZh4Bqg-TS","0jJSoI9gg9vc","UD8yqbFoeBeu","1SXQJI6JlNRB","vOVybMiTZw_x"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dXlcPic1uX77","colab_type":"text"},"source":["if both packages are in your import path (sys.path), and the module/class you want is in example/example.py, then to access the class without relative import try: from example.example import fkt\n","\n","if __name__ == '__main__':\n","    from mymodule import as_int\n","else:\n","    from .mymodule import as_int"]},{"cell_type":"markdown","metadata":{"id":"L09Z9X79zHn6","colab_type":"text"},"source":["https://stackoverflow.com/questions/448271/what-is-init-py-for\n","\n","Files named __init__.py are used to mark directories on disk as Python package directories. If you have the files\n","\n","mydir/spam/__init__.py\n","mydir/spam/module.py\n","and mydir is on your path, you can import the code in module.py as\n","\n","import spam.module\n","or\n","\n","from spam import module"]},{"cell_type":"markdown","metadata":{"id":"KuI-fu5UrvlK","colab_type":"text"},"source":["## Intro"]},{"cell_type":"markdown","metadata":{"id":"oRPCZJ7gcW78","colab_type":"text"},"source":["Inro\n","\n","CLI CLIARGS > \n","MAIN> \n","SPIDER > MINER > CLEANER"]},{"cell_type":"markdown","metadata":{"id":"vUWmBoZZfhOc","colab_type":"text"},"source":["# Background & Resources"]},{"cell_type":"markdown","metadata":{"id":"1PDCakVVdzPg","colab_type":"text"},"source":["[MD Access to Judicial Records](https://govt.westlaw.com/mdc/Browse/Home/Maryland/MarylandCodeCourtRules?guid=NDF9067D03C9711E6B2B6BCAD65966614&originationContext=documenttoc&transitionType=Default&contextData=(sc.Default) )\n","\n","mdcourts \n","- [faq](https://mdcourts.gov/casesearch2/faq)\n","- [termsdisclaimer](https://mdcourts.gov/reference/termsdisclaimer)\n","- [estatesearchglossary](https://mdcourts.gov/casesearch2/estatesearchglossary)"]},{"cell_type":"markdown","metadata":{"id":"wMbm7iGFeHf9","colab_type":"text"},"source":["Other Helpful search tips:\n","\n","http://www.registers.state.md.us. EstateSearch provides public Internet access to information from estate records maintained by the Maryland Registers of Wills. This information includes decedent’s name, estate number and status, date of death, date of filing, personal representative, attorney, decedent alias, and docket history. All records are available from 1998 to present for all estates filed with the Register of Wills’ office. The data is updated daily at the end of the business day.\n","\n","Use a % as a wildcard when searching in a field (Smith%) would give you all names that start with Smith, Smithson, Smithsburg, Smithman, etc.\n","When searching for a date range you need to enter a last name or first name (partials allowed)\n","You can sort the columns by clicking on the column header.\n","Click the Search again option to take you back to your previous search criteria.\n","Use the clear button to clear all fields and begin your search again.\n","\n","List of Estate Types\n","\n","- Regular Estate (RE) - Assets subject to administration in excess of $30,000 ($50,000 if the spouse is the sole legatee or heir).\n","- Regular Estate Judicial (RJ) - A proceeding conducted by the Orphans' Court when matters cannot be handled administratively. For example, when the validity of the will is at issue, or the will is lost, stolen or damaged.\n","- Small Estate (SE) - Assets subject to administration valued at $30,000 or less ($50,000 if the spouse is the sole legatee or heir).\n","-Small Estate Judicial (SJ) - A proceeding conducted by the Orphans' Court when matters cannot be handled administratively. For example, when the validity of the will is at issue, or the will is lost, stolen or damaged.\n","- Foreign Proceeding (FP) - Decedent domiciled out of state with real property in Maryland.\n","- Motor Vehicle (MV) - Transfer of motor vehicle only.\n","- NonProbate (NP) - Property of the decedent which passes by operation of law such as a joint tenancy, tenants by the entireties, or property passing under a deed or trust, revocable or irrevocable. Non probate property must be reported to the Register of Wills on the Information Report or Application to Fix Inheritance Tax on Non-Probate assets.\n","- Unprobated Will Only (UN) - Will and Information Report filed with will and/or Application to Fix Inheritance Tax.\n","- Modified Administration (MA) - A procedure available when the residual legatees consists of the personal representative, spouse; and children. Estate is solvent, Final Distribution can occur within 12 months from date of appointment. A verified final report is filed within 10 months from the date of appointment.\n","- Guardianship Estate (GE) - Guardianship of property for a minor.\n","- Limited Order (LO) – A limited order to locate assets or a will."]},{"cell_type":"markdown","metadata":{"id":"GAn329L8ez8j","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HJihPyPhqP8x","colab":{}},"source":["%%capture\n","! pip install mechanicalsoup\n","! pip install urlopen"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YZu_L7W2-UAE","colab_type":"code","outputId":"10a69045-f1e0-4abe-e2a7-d7c7b50d6fd1","executionInfo":{"status":"ok","timestamp":1583777518061,"user_tz":240,"elapsed":423,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQg4PIuuYRyIyCWtHJXvMzkkkBSQpEzUYL20Bjog=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import mechanicalsoup\n","mechanicalsoup.__version__"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0.12.0'"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"3FKzvamyRP3b","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1lPX9Lk605WF","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"b74ce1d1-719c-479b-92db-ce64c8ad240a","executionInfo":{"status":"ok","timestamp":1583777047889,"user_tz":240,"elapsed":444,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQg4PIuuYRyIyCWtHJXvMzkkkBSQpEzUYL20Bjog=s64","userId":"06248540216665543849"}}},"source":["# For relative imports to work in Python 3.6\n","import os, sys; \n","os.path.realpath('./') \n","sys.path.append(os.path.dirname(os.path.realpath('/gdrive/My Drive/colabs/close_crawl/modules/cleaner.py')))\n","sys.path"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," '/env/python',\n"," '/usr/lib/python36.zip',\n"," '/usr/lib/python3.6',\n"," '/usr/lib/python3.6/lib-dynload',\n"," '/usr/local/lib/python3.6/dist-packages',\n"," '/usr/lib/python3/dist-packages',\n"," '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n"," '/root/.ipython',\n"," '/gdrive/My Drive/colabs/close_crawl/modules']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"y_f1mURJojFF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"4e7cace0-fbe9-4388-84aa-f89355e0cc7c","executionInfo":{"status":"ok","timestamp":1583777049305,"user_tz":240,"elapsed":375,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQg4PIuuYRyIyCWtHJXvMzkkkBSQpEzUYL20Bjog=s64","userId":"06248540216665543849"}}},"source":["cd 'My Drive'/colabs/close_crawl"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'My Drive/colabs/close_crawl'\n","/gdrive/My Drive/colabs/close_crawl\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EsBZBtdoRWxa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"a7786496-c9ba-4e1b-9b43-fe743bd18ac5","executionInfo":{"status":"ok","timestamp":1583777051295,"user_tz":240,"elapsed":1491,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQg4PIuuYRyIyCWtHJXvMzkkkBSQpEzUYL20Bjog=s64","userId":"06248540216665543849"}}},"source":["ls"],"execution_count":12,"outputs":[{"output_type":"stream","text":[" checkpoint.json      foreclosures_2018_closecrawl.csv   \u001b[0m\u001b[01;34mresponses\u001b[0m/\n"," cliargs.py           \u001b[01;34mmodules\u001b[0m/                           _version.py\n"," cli.py               no_case.json\n","'Close Crawl.ipynb'   \u001b[01;34m__pycache__\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hCSp69Dou6tF","colab_type":"text"},"source":["## Scrape"]},{"cell_type":"markdown","metadata":{"id":"tkrJRvsCwL4E","colab_type":"text"},"source":["### Local Browser"]},{"cell_type":"code","metadata":{"id":"SFC5Kz6gyEcG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"a0c93e18-ef99-4cc5-b941-068c7cd771d6","executionInfo":{"status":"ok","timestamp":1583777051297,"user_tz":240,"elapsed":398,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQg4PIuuYRyIyCWtHJXvMzkkkBSQpEzUYL20Bjog=s64","userId":"06248540216665543849"}}},"source":["\"\"\"local_browser\n","\n","NOTICE: Close Crawl runs its browser form submissions through Mechanize.\n","The module, however, is deprecated and does not support Python 3. The more\n","stable and maintained Mechanize and BeautifulSoup wrapper, MechanicalSoup,\n","will be replacing the Mechanize methods to support Python 3.\n","\n","This module contains the configurations and settings for the browser used for\n","crawling and scraping through the pages in Close Crawl. The script contains the\n","implementation of the Session class which inherits attributes from the classobj\n","mechanize.Browser()\n","\n","The script works as an internal module for Close Crawl, but can be imported\n","as a module for testing purposes.\n","\n","TODO:\n","    Replace deprecated Mechanize with MechanicalSoup\n","    Fork Mechanize to support Python3\n","\n","\"\"\""],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'local_browser\\n\\nNOTICE: Close Crawl runs its browser form submissions through Mechanize.\\nThe module, however, is deprecated and does not support Python 3. The more\\nstable and maintained Mechanize and BeautifulSoup wrapper, MechanicalSoup,\\nwill be replacing the Mechanize methods to support Python 3.\\n\\nThis module contains the configurations and settings for the browser used for\\ncrawling and scraping through the pages in Close Crawl. The script contains the\\nimplementation of the Session class which inherits attributes from the classobj\\nmechanize.Browser()\\n\\nThe script works as an internal module for Close Crawl, but can be imported\\nas a module for testing purposes.\\n\\nTODO:\\n    Replace deprecated Mechanize with MechanicalSoup\\n    Fork Mechanize to support Python3\\n\\n'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"zXKAJrfLyClo","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, print_function, unicode_literals\n","# import cookielib \n","import http.cookiejar as cookielib # for Python3\n","import warnings\n","from urllib.request import urlopen\n","# from urllib import urlopen urllib.request\n","\n","## from mechanize import Browser, _http\n","import mechanicalsoup\n","\n","from settings import HEADER, URL\n","\n","warnings.filterwarnings(\"ignore\", category=UserWarning)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ACGn1Rr2wNoB","colab_type":"code","colab":{}},"source":["class Session(object):\n","\n","    def __init__(self):\n","        \"\"\"Constructor\n","\n","        Args:\n","            None\n","\n","        Attributes:\n","            browser (`mechanize._mechanize.Browser`): browser object in session\n","        \"\"\"\n","\n","        self.browser = mechanicalsoup.StatefulBrowser()\n","\n","        # set error and debug handlers for the browser\n","\n","        # cookie jar\n","        self.browser.set_cookiejar(cookielib.LWPCookieJar())\n","\n","        # browser options\n","        # self.browser.set_handle_equiv(True)\n","        # self.browser.set_handle_gzip(True)\n","        # self.browser.set_handle_redirect(True)\n","        # self.browser.set_handle_referer(True)\n","        # self.browser.set_handle_robots(False)\n","\n","        # follows refresh 0 but doesn't hang on refresh > 0\n","        #self.browser.set_handle_refresh( _http.HTTPRefreshProcessor(), max_time=1 )\n","\n","        # user-Agent\n","        # self.browser.addheaders = [(\"User-agent\", HEADER)]\n","\n","    def close(self):\n","        \"\"\"Destructor for Session. Closes current browser session\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        self.browser.close()\n","\n","    def case_id_form(self, case):\n","        \"\"\"Grabs the form in the case searching page, and inputs the\n","        case number to return the response.\n","\n","        Args:\n","            case (`str`): case ID to be scraped\n","\n","        Returns:\n","            response (`str`): HTML response\n","        \"\"\"\n","\n","        # iterate through the forms to find the correct one\n","        #for form in self.browser.forms():\n","        #    if form.attrs[\"name\"] == \"inquiryFormByCaseNum\":\n","        #        self.browser.form = form\n","        #        break\n","        \n","        self.browser.select_form('form[action=\"/casesearch/inquiryByCaseNum.jis\"]') \n","\n","        # submit case ID and return the response\n","        self.browser[\"caseId\"] = case\n","        response = self.browser.submit_selected()\n","        response = response.text\n","        # if any( case_type in response.upper() for case_type in (\"FORECLOSURE\", \"FORECLOSURE RIGHTS OF REDEMPTION\", \"MOTOR TORT\") ): print (response.upper)\n","\n","        self.browser.open(\"http://casesearch.courts.state.md.us/casesearch/inquiryByCaseNum.jis\")\n","        # , \"MOTOR TORT\"\n","        return response if any(\n","            case_type in response.upper() for case_type in\n","            (\"FORECLOSURE\", \"FORECLOSURE RIGHTS OF REDEMPTION\")\n","        ) else False\n","\n","    def disclaimer_form(self):\n","        \"\"\"Navigates to the URL to proceed to the case searching page\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","\n","        # visit the site\n","        print(URL)\n","        self.browser.open(\"http://casesearch.courts.state.md.us/casesearch/\")\n","\n","        # select the only form on the page\n","        self.browser.select_form('form')\n","\n","        # select the checkbox\n","        self.browser[\"disclaimer\"] = ['Y']\n","\n","        # submit the form\n","        self.browser.submit_selected()\n","\n","    @staticmethod\n","    def server_running():\n","        \"\"\"Checks the status of the Casesearch servers\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            `True` if server is up, `False` otherwise\n","        \"\"\"\n","        return urlopen(URL).getcode() == 200\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uchYLV629hp6","colab_type":"text"},"source":["### Spider"]},{"cell_type":"code","metadata":{"id":"EdHlEWvfEn7R","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, print_function, unicode_literals\n","from json import dumps, load\n","from os import path, makedirs\n","from random import uniform\n","import sys\n","from time import sleep\n","\n","from tqdm import trange\n","\n","# from local_browser import Session\n","from settings import CASE_PAT, CHECKPOINT, HTML_DIR, HTML_FILE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qX1yv91yEp3x","colab_type":"code","colab":{}},"source":["class Spider(object):\n","\n","    def __init__(self, case_type, year, bounds=range(1, 6), gui=False):\n","\n","        # initial disclaimer page for terms and agreements\n","        self.browser = Session()\n","\n","        if not self.browser.server_running():\n","            sys.exit(\"Server is unavailable at the moment\")\n","\n","        print (self.browser)\n","        \n","        self.browser.disclaimer_form()\n","\n","        self.WAITING_TIME = 0\n","        self.case_type = case_type\n","        self.year = year\n","        self.bounds = bounds\n","\n","        if not path.exists(HTML_DIR):\n","            makedirs(HTML_DIR)\n","\n","    def save_response(self):\n","\n","        case_range = trange(\n","            len(self.bounds), desc=\"Crawling\", leave=True\n","        )\n","\n","        for case_num in case_range:\n","\n","            if case_num and not case_num % 500:\n","                print(\"500 CASES SCRAPED. SCRIPT WILL WAIT 5 MINUTES TO RESUME\")\n","\n","                for i in range(300, 0, -1):\n","                    sleep(1)\n","                    sys.stdout.write('\\r' + \"%02d:%02d\" % divmod(i, 60))\n","                    sys.stdout.flush()\n","\n","            case = CASE_PAT.format(\n","                type=self.case_type,\n","                year=self.year,\n","                num=\"{:04d}\".format(int(str(self.bounds[case_num])[-4:]))\n","            )\n","\n","            try:\n","\n","                wait = uniform(0.0, 0.5)\n","                sleep(wait)\n","\n","                self.WAITING_TIME += wait\n","\n","                case_range.set_description(\"Crawling {}\".format(case))\n","\n","                stripped_html = self.browser.case_id_form(case)\n","                print('returend this ' , stripped_html)\n","\n","                if stripped_html:\n","                    with open(\n","                        HTML_FILE.format(case=case) + \".html\", 'w'\n","                    ) as case_file:\n","                        case_file.write(str(stripped_html))\n","\n","            # pause process\n","            except KeyboardInterrupt:\n","\n","                self.dump_json({\n","                    \"error_case\":\n","                        \"{:04d}\".format(int(str(self.bounds[case_num])[-4:])),\n","                        \"year\": self.year,\n","                        \"type\": self.type\n","                })\n","                print(\"Crawling paused at\", case)\n","                break\n","\n","            # case does not exist\n","            except IndexError:\n","\n","                self.dump_json({\"error_case\": case})\n","                print(case, \"does not exist\")\n","                break\n","\n","        # close browser and end session\n","        self.close_sesh()\n","\n","    @staticmethod\n","    def dump_json(data):\n","\n","        with open(CHECKPOINT, \"r+\") as checkpoint:\n","            checkpoint_data = load(checkpoint)\n","\n","            for key, val in data.items():\n","                checkpoint_data[key] = val\n","\n","            checkpoint_data[key] = data\n","            checkpoint.seek(0)\n","            checkpoint.write(dumps(checkpoint_data))\n","            checkpoint.truncate()\n","\n","    def close_sesh(self):\n","        self.browser.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gh9MXtpeCOFq","colab_type":"code","colab":{}},"source":["scrape = True\n","if scrape:\n","  spider = Spider(\n","    case_type='C', year=16,\n","    bounds=range(6000, 6200), gui=False\n","  )\n","  spider.save_response()\n","  wait = spider.WAITING_TIME"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrwQHlZWg4mu","colab_type":"text"},"source":["# Miner"]},{"cell_type":"markdown","metadata":{"id":"-yoZh4Bqg-TS","colab_type":"text"},"source":["## Patterns"]},{"cell_type":"code","metadata":{"id":"t6NqQ75FhiGF","colab_type":"code","outputId":"648264d5-9833-423a-a5a2-f33f07648201","executionInfo":{"status":"ok","timestamp":1583777067494,"user_tz":240,"elapsed":357,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQg4PIuuYRyIyCWtHJXvMzkkkBSQpEzUYL20Bjog=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["\"\"\"Patterns\n","\n","Regular expression patterns and string filtering functions implemented in the\n","project. This file is intended to only be used as a non-executable script.\n","\n","TODO:\n","    Finish docs\n","\n","(\\d{1,4}\\s[\\w\\s]{1,20}((?:st(reet)?|ln|lane|ave(nue)?|r(?:oa)?d|highway|hwy|\n","dr(?:ive)?|sq(uare)?|tr(?:ai)l|c(?:our)?t|parkway|pkwy|cir(cle)?|ter(?:race)?|\n","boulevard|blvd|pl(?:ace)?)\\W?(?=\\s|$))(\\s(apt|block|unit)\\W?([A-Z]|\\d+))?)\n","\"\"\""],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Patterns\\n\\nRegular expression patterns and string filtering functions implemented in the\\nproject. This file is intended to only be used as a non-executable script.\\n\\nTODO:\\n    Finish docs\\n\\n(\\\\d{1,4}\\\\s[\\\\w\\\\s]{1,20}((?:st(reet)?|ln|lane|ave(nue)?|r(?:oa)?d|highway|hwy|\\ndr(?:ive)?|sq(uare)?|tr(?:ai)l|c(?:our)?t|parkway|pkwy|cir(cle)?|ter(?:race)?|\\nboulevard|blvd|pl(?:ace)?)\\\\W?(?=\\\\s|$))(\\\\s(apt|block|unit)\\\\W?([A-Z]|\\\\d+))?)\\n'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"38X8chjVhjL0","colab_type":"code","colab":{}},"source":["from re import compile as re_compile\n","from re import I as IGNORECASE\n","from string import punctuation"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqPf-3a7g9R9","colab_type":"code","colab":{}},"source":["PUNCTUATION = punctuation.replace('#', '')  # all punctuations except '#'\n","\n","street_address = re_compile(\n","    \"(\"  # begin regex group\n","    \"\\d{1,4}\\s\"  # house number\n","    \"[\\w\\s]{1,20}\"  # street name\n","    \"(\"  # start street type group\n","    \"(?:st(reet)?|ln|lane|ave(nue)?\"  # (st)reet, lane, ln, (ave)nue\n","    \"|r(?:oa)?d|highway|hwy|dr(?:ive)?\"  # rd, road, hwy, highway, (dr)ive\n","    \"|sq(uare)?|tr(?:ai)l|c(?:our)?t\"  # (sq)uare, (tr)ail, ct, court\n","    \"|parkway|pkwy|cir(cle)?|ter(?:race)?\"  # parkway, pkwy, (cir)cle, (ter)race\n","    \"|boulevard|blvd|pl(?:ace)?\"  # boulevard, bvld, (pl)ace\n","    \"\\W?(?=\\s|$))\"  # look ahead for whitespace or end of string\n","    \")\"  # end street type group\n","    \"(\\s(apt|block|unit)(\\W|#)?([\\d|\\D|#-|\\W])+)?\"  # apt, block, unit number\n","    \")\",  # end regex group\n","    IGNORECASE  # case insensitive flag\n",")\n","\n","# case insensitive delimiter for Titles\n","TITLE_SPLIT_PAT = re_compile(\" vs \", IGNORECASE)\n","\n","# pattern for Baltimore zip codes\n","ZIP_STR = \"2\\d{4}\"\n","ZIP_PAT = re_compile(ZIP_STR)\n","\n","# regex pattern to capture monetary values between $0.00 and $999,999,999.99\n","# punctuation insensitive\n","MONEY_STR = \"\\$\\d{,3},?\\d{,3},?\\d{,3}\\.?\\d{2}\"\n","MONEY_PAT = re_compile(MONEY_STR)\n","\n","NULL_ADDR = re_compile(\n","    \"^(\"\n","    \"(\" + MONEY_STR + \")\"\n","    \"|(\" + ZIP_STR + \")\"\n","    \"|(\\d+)\"\n","    \"|(\" + ZIP_STR + \".*\" + MONEY_STR + \")\"\n","    \")$\",\n","    IGNORECASE\n",")\n","\n","STRIP_ADDR = re_compile(\n","    \"(balto|\" + ZIP_STR + \"|md|\" + MONEY_STR + \").*\",\n","    IGNORECASE\n",")\n","\n","\n","def filter_addr(address):\n","\n","    try:\n","      return ''.join(\n","        street_address.search( \n","          address.translate( str.maketrans('','', PUNCTUATION) ) \n","        ).group(0)\n","      )\n","    except AttributeError:\n","      return ''\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0jJSoI9gg9vc","colab_type":"text"},"source":["## Miner"]},{"cell_type":"code","metadata":{"id":"UTIs0auQhUbc","colab_type":"code","outputId":"a9c8e8bf-dcba-4be7-d74b-623dcd1a10a3","executionInfo":{"status":"ok","timestamp":1583777070242,"user_tz":240,"elapsed":367,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQg4PIuuYRyIyCWtHJXvMzkkkBSQpEzUYL20Bjog=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["\"\"\"Miner\n","\n","\n","TODO:\n","    Finish docs\n","    Refactor to lower complexity\n","\n","\"\"\""],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Miner\\n\\n\\nTODO:\\n    Finish docs\\n    Refactor to lower complexity\\n\\n'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"Ki2aj3vGhQNc","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, print_function, unicode_literals\n","from csv import DictWriter\n","from json import dump, dumps, load\n","from os import path\n","\n","from bs4 import BeautifulSoup\n","from tqdm import trange\n","\n","# from patterns import MONEY_PAT, TITLE_SPLIT_PAT, ZIP_PAT, filter_addr\n","from settings import HTML_FILE, NO_CASE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkKl_W0Og9ls","colab_type":"code","colab":{}},"source":["# data mining settings\n","FEATURES = [\n","    \"Filing Date\",\n","    \"Case Number\",\n","    \"Case Type\",\n","    \"Title\",\n","    \"Plaintiff\",\n","    \"Defendant\",\n","    \"Address\",\n","    \"Business or Organization Name\",\n","    \"Party Type\",\n","]\n","FIELDS = FEATURES + [ \"Zip Code\", \"Partial Cost\" ]\n","INTERNAL_FIELDS = [ \"Business or Organization Name\", \"Party Type\"]\n","\n","class Miner(object):\n","\n","    def __init__(self, responses, output, debug=False):\n","\n","        self.responses = responses\n","        self.output = output\n","        self.debug = debug\n","        self.dataset = []\n","        self.maybe_tax = False\n","        self.features = [i + ':' for i in FEATURES]\n","\n","    def scan_files(self):\n","\n","        case_range = trange(len(self.responses), desc=\"Mining\", leave=True) \\\n","            if not self.debug else range(len(self.responses))\n","\n","        for file_name in case_range:\n","            with open(\n","                HTML_FILE.format(case=self.responses[file_name]), 'r'\n","            ) as html_src:\n","\n","                if not self.debug:\n","                    case_range.set_description(\n","                        \"Mining {}\".format(self.responses[file_name])\n","                    )\n","\n","                feature_list = self.scrape(html_src.read())\n","                row = self.distribute(feature_list)\n","\n","                if not row:\n","\n","                    if not path.isfile(NO_CASE):\n","                        with open(NO_CASE, 'w') as no_case_file:\n","                            dump([], no_case_file)\n","\n","                    with open(NO_CASE, \"r+\") as no_case_file:\n","                        no_case_data = load(no_case_file)\n","                        no_case_data.append(str(self.responses[file_name][:-5]))\n","                        no_case_file.seek(0)\n","                        no_case_file.write(dumps(sorted(set(no_case_data))))\n","                        no_case_file.truncate()\n","\n","                self.dataset.extend(row)\n","\n","    def export(self):\n","\n","        file_exists = path.isfile(self.output)\n","\n","        with open(self.output, 'a') as csv_file:\n","            writer = DictWriter(\n","                csv_file,\n","                fieldnames=[\n","                    col for col in FIELDS if col not in INTERNAL_FIELDS\n","                ]\n","            )\n","\n","            if not file_exists:\n","                writer.writeheader()\n","\n","            for row in self.dataset:\n","                writer.writerow(row)\n","\n","    def scrape(self, html_data):\n","        \"\"\"Scrapes the desired features\n","\n","        Args:\n","            html_data: <str>, source HTML\n","\n","        Returns:\n","            scraped_features: <dict>, features scraped and mapped from content\n","        \"\"\"\n","\n","        soup = BeautifulSoup(html_data, \"html.parser\")\n","\n","        # Search for the word 'tax in the document'\n","        if \"tax\" in soup.text.lower():\n","            self.maybe_tax = True\n","\n","        # Create an array from all TR's with the inner HTML for each TR\n","        # Data we want is stored inside an arbitrary # of 'span' tags inside the TR's.\\\n","        tr_list = soup.find_all(\"tr\")\n","\n","        # This will create an array for each TR with an array of SPAN values inside.  \n","        feature_list = []\n","        for tag in tr_list:\n","            try:\n","                # Create an innerhtml array for all spans within a single TR\n","                tag = [j.string for j in tag.findAll(\"span\")]\n","                if set(tuple(tag)) & set(self.features):\n","                    try:\n","                        # Save the spans inner HTML if its not a header label\n","                        tag = [i for i in tag if \"(each\" not in i.lower()]\n","                    except AttributeError:\n","                        continue\n","                    feature_list.append(tag)\n","\n","            except IndexError:\n","                continue\n","\n","        # feature_list is an array [tr] of arrays [spans]. we want this flattened.\n","        # [tr1span1KEY, tr1span1VALUE, tr1span2KEY, tr1span2VALUE, tr2span1KEY, tr2span1VALUE, ]\n","        try:\n","            # flatten multidimensional list\n","            feature_list = [\n","                item.replace(':', '')\n","                for sublist in feature_list for item in sublist\n","            ]\n","\n","        except AttributeError:\n","            pass\n","\n","        return feature_list\n","\n","    def distribute(self, feature_list):\n","\n","        # feature_list ~= [html][tr][spans].innterHTML\n","        # [tr1span1KEY, tr1span1VALUE, tr1span2KEY, tr1span2VALUE, tr2span1KEY, tr2span1VALUE, ]\n","        def __pair(list_type):\n","\n","            # break up elements with n-tuples greater than 2\n","            def __raw_business(i): return any( x in feature_list[i:i + 2][0] for x in INTERNAL_FIELDS )\n","            def __feature_list(i): return feature_list[i:i + 2][0] in FEATURES\n","            condition = __raw_business if list_type else __feature_list\n","\n","            # then convert list of tuples to dict for faster lookup\n","            return [ tuple(feature_list[i:i + 2]) for i in range(0, len(feature_list), 2) if condition(i) ]\n","\n","        raw_business = __pair(1) # [(x1,y1),(x2,y2),(x3,y3)] => INTERNAL_FIELDS\n","        feature_list = dict(__pair(0)) # FEATURES\n","        filtered_business = []\n","\n","        # Party_Type = 'property address' not 'plaintiff' or 'defendant'\n","        # Input exists 'Business or Org Name' and == an Address\n","        for label, value in enumerate(raw_business):\n","            try:\n","                party_type = value[1].upper()\n","                section = raw_business[label + 1][0].upper()\n","                flag1 = party_type == \"PROPERTY ADDRESS\"\n","                flag2 = section == \"BUSINESS OR ORGANIZATION NAME\"\n","                if flag1 and flag2: filtered_business.append(raw_business[label + 1])\n","\n","            except IndexError:\n","                print(\"Party Type issue at Case\", feature_list[\"Case Number\"])\n","\n","        scraped_features = []\n","\n","        for address in filtered_business:\n","\n","            str_address = filter_addr(str(address[-1]))\n","            \n","            temp_features = {\n","                key: value for key, value in feature_list.items()\n","                if key in [\"Title\", \"Case Type\", \"Case Number\", \"Filing Date\"]\n","            }\n","\n","\n","            if temp_features[\"Case Type\"].upper() == \"FORECLOSURE\":\n","                temp_features[\"Case Type\"] = \"Mortgage\"\n","\n","            elif temp_features[\"Case Type\"].upper() == \\\n","                    \"FORECLOSURE RIGHTS OF REDEMPTION\" and self.maybe_tax:\n","                temp_features[\"Case Type\"] = \"Tax\"\n","\n","            else:\n","                # break out of the rest of the loop if case type is neither\n","                continue\n","\n","            if 'Title' not in temp_features: \n","              # print('feature_list');\n","              # print(feature_list);\n","              # print('\\n \\n raw_business');\n","              # print(raw_business);\n","              continue\n","\n","            # break up Title feature into Plaintiff and Defendant\n","            try:\n","                temp_features[\"Plaintiff\"], temp_features[\"Defendant\"] = \\\n","                    TITLE_SPLIT_PAT.split(temp_features[\"Title\"])\n","\n","            except ValueError:\n","                temp_features[\"Plaintiff\"], temp_features[\"Defendant\"] = \\\n","                    (\", \")\n","\n","            temp_features[\"Address\"] = \\\n","                str_address if str_address else address[-1]\n","\n","            temp_features[\"Zip Code\"] = ''.join(ZIP_PAT.findall(address[-1]))\n","\n","            temp_features[\"Partial Cost\"] = ''.join(\n","                MONEY_PAT.findall(address[-1])\n","            )\n","\n","            scraped_features.append(temp_features)\n","            temp_features = {}\n","\n","        return scraped_features\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hiqx7Ue0iIyt","colab_type":"code","colab":{}},"source":["from settings import CHECKPOINT, HTML_DIR\n","from os import path, remove, walk\n","\n","temp_output = \"temp_data.csv\"\n","file_array = [filenames for (dirpath, dirnames, filenames) in walk(HTML_DIR)][0]\n","top5 = file_array[:1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lh-gG37rjC1E","colab_type":"code","colab":{}},"source":["miner = Miner(file_array, temp_output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4s7NRF5CjEI-","colab_type":"code","colab":{}},"source":["miner.scan_files()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KW1OhQPvjFKY","colab_type":"code","colab":{}},"source":["miner.export()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkxm-KEs0Ahb","colab_type":"code","colab":{}},"source":["miner.output = 'tem_dat.csv'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"acePSN2lzMzk","colab_type":"code","outputId":"63f6b62c-e783-44be-c822-768fef53542c","executionInfo":{"status":"ok","timestamp":1574706371809,"user_tz":300,"elapsed":322,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["miner.output"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'tem_dat.csv'"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"qWW5dmr5bFGh","colab_type":"code","outputId":"91d8c54f-029d-49c3-d70c-b90187b502a4","executionInfo":{"status":"ok","timestamp":1574703163569,"user_tz":300,"elapsed":341,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":232}},"source":["scrapethisfile = '24O19000982.html'\n","with open( HTML_FILE.format(case=scrapethisfile), 'r' ) as html_src:\n","  print( scrapethisfile);\n","  feature_list = scrape(html_src.read())\n","  print(feature_list);\n","  row = distribute(feature_list)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["24O19000982.html\n","['Case Number', '24O19000982', 'Case Type', 'Foreclosure', 'Filing Date', '06/13/2019', 'Party Type', 'Plaintiff', 'Party No.', '1', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Defendant', 'Party No.', '1', 'Address', '3818 Glenarm Avenue', 'Party Type', 'Defendant', 'Party No.', '2', 'Address', '3818 Glenarm Avenue', 'Party Type', 'Property Address', 'Party No.', '1', 'Business or Organization Name', '3818 Glenarm Avenue ($98,872.62)', 'Party Type', 'Trustee', 'Party No.', '1', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '2', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '6', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '3', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '4', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '5', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '7', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '8', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '9', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '10', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '11', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '12', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '13', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '14', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Trustee', 'Party No.', '15', 'Address', '6003 Executive Blvd', 'Address', '6003 Executive Blvd', 'Party Type', 'Plaintiff', 'Party No.', '1', 'Party Type', 'Plaintiff', 'Party No.', '1', 'Party Type', 'Plaintiff', 'Party No.', '1', 'Party Type', 'Plaintiff', 'Party No.', '1']\n","\n"," [('Party Type', 'Plaintiff'), ('Party Type', 'Defendant'), ('Party Type', 'Defendant'), ('Party Type', 'Property Address'), ('Business or Organization Name', '3818 Glenarm Avenue ($98,872.62)'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Trustee'), ('Party Type', 'Plaintiff'), ('Party Type', 'Plaintiff'), ('Party Type', 'Plaintiff'), ('Party Type', 'Plaintiff')]\n","\n"," {'Case Number': '24O19000982', 'Case Type': 'Foreclosure', 'Filing Date': '06/13/2019', 'Party Type': 'Plaintiff', 'Address': '6003 Executive Blvd', 'Business or Organization Name': '3818 Glenarm Avenue ($98,872.62)'}\n","Party Type issue at Case\n","feature_list\n","{'Case Number': '24O19000982', 'Case Type': 'Foreclosure', 'Filing Date': '06/13/2019', 'Party Type': 'Plaintiff', 'Address': '6003 Executive Blvd', 'Business or Organization Name': '3818 Glenarm Avenue ($98,872.62)'}\n","\n"," \n"," raw_business\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UD8yqbFoeBeu","colab_type":"text"},"source":["## Manual"]},{"cell_type":"code","metadata":{"id":"B-p8n98beB2w","colab_type":"code","colab":{}},"source":["soup = ''\n","scrapethisfile = '24O19000982.html'\n","with open( HTML_FILE.format(case=scrapethisfile), 'r' ) as html_src:\n","  souptxt = html_src.read()\n","  soup = BeautifulSoup(souptxt, \"html.parser\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UEB_C0FA1vu7","colab":{}},"source":["from settings import CHECKPOINT, HTML_DIR\n","from os import path, remove, walk\n","\n","temp_output = \"temp_dat.csv\"\n","file_array = [filenames for (dirpath, dirnames, filenames) in walk(HTML_DIR)][0]\n","top5 = [ file_array[41] ]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"suTSJGq11vu9","colab":{}},"source":["miner = Miner(top5, temp_output)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bXHOxByXmi0","colab_type":"code","colab":{}},"source":["miner.scan_files()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K1OUa3XztaOg","colab_type":"text"},"source":["# Cleaner"]},{"cell_type":"code","metadata":{"id":"tMkruUBVtEmu","colab_type":"code","outputId":"e9769fde-13db-4036-fd0b-c884ffd4e7dd","executionInfo":{"status":"ok","timestamp":1574706476759,"user_tz":300,"elapsed":398,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["\"\"\"Cleaner\n","\n","This module implements post-scraping cleaning processes on the raw initial\n","dataset. Processes include stripping excess strings off Address values,\n","removing Zip Code and Partial Cost values mislabeled as Address, and merging\n","rows containing blank values in alternating features.\n","\n","The script works as an internal module for Close Crawl, but can be executed\n","as a standalone to manually process datasets:\n","\n","    $ python cleaner.py <path/to/old/dataset> <path/of/new/dataset>\n","\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Cleaner\\n\\nThis module implements post-scraping cleaning processes on the raw initial\\ndataset. Processes include stripping excess strings off Address values,\\nremoving Zip Code and Partial Cost values mislabeled as Address, and merging\\nrows containing blank values in alternating features.\\n\\nThe script works as an internal module for Close Crawl, but can be executed\\nas a standalone to manually process datasets:\\n\\n    $ python cleaner.py <path/to/old/dataset> <path/of/new/dataset>\\n\\n'"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"code","metadata":{"id":"FJhRzfR6tdKe","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, print_function, unicode_literals\n","from pandas import DataFrame, concat, read_csv, to_datetime\n","# from patterns import NULL_ADDR, STRIP_ADDR, filter_addr, punctuation"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7TtTB-UtiPw","colab_type":"code","colab":{}},"source":["class Cleaner(object):\n","    \"\"\"Class object for cleaning the raw dataset extracted after the initial\n","    scraping\n","    \"\"\"\n","\n","    def __init__(self, path):\n","        \"\"\"Constructor for Cleaner\n","\n","        Args:\n","            path (`str`): path to input CSV dataset\n","\n","        Attributes:\n","            df (`pandas.core.frame.DataFrame`): initial DataFrame\n","            columns (`list` of `str`): columns of the DataFrame\n","            clean_df (`pandas.core.frame.DataFrame`): final DataFrame to be\n","                outputted\n","        \"\"\"\n","\n","        self.df = self.prettify(read_csv(path))\n","\n","        self.columns = list(self.df)\n","        self.clean_df = []\n","\n","    @staticmethod\n","    def prettify(df, internal=True):\n","        \"\"\"Drops duplicates, sorts and fills missing values in the DataFrame\n","        to make it manageable.\n","\n","        Args:\n","            df (`pandas.core.frame.DataFrame`): DataFrame to be managed\n","            internal (`bool`, optional): flag for determining state of\n","                DataFrame\n","\n","        Returns:\n","            df (`pandas.core.frame.DataFrame`): organized DataFrame\n","        \"\"\"\n","\n","        df.drop_duplicates(inplace=True, keep=False)\n","        df[\"Filing Date\"] = to_datetime(df[\"Filing Date\"])\n","\n","        df.sort_values(\n","            [\"Filing Date\", \"Case Number\", \"Address\"],\n","            ascending=[True] * 3,\n","            inplace=True\n","        )\n","\n","        if internal:\n","            df[\"Zip Code\"] = df[\"Zip Code\"].fillna(0.0).astype(int)\n","            df[\"Zip Code\"] = df[\"Zip Code\"].replace(0, '')\n","\n","        return df\n","\n","    def clean_addr(self):\n","        \"\"\"Cleans excess strings off Address values and removes Zip Code and\n","        Partial Cost values mislabeled as Address.\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","\n","        def clean_string(addr):\n","            \"\"\"Applies regular expressions and other filters on Address\n","            values\n","\n","            Args:\n","                addr (`str`): Address value to be filtered\n","\n","            Returns:\n","                addr (`str`): filtered Address value\n","            \"\"\"\n","\n","            # if value does not match the street_address pattern\n","            if not filter_addr(addr):  # patterns.filter_addr\n","\n","                if NULL_ADDR.sub('', addr):  # value may contain valid Address\n","                    return str(\n","                        STRIP_ADDR.sub(\n","                            '', addr)  # strip off Zip Code and Partial Cost\n","                    ).translate(\n","                        {ord(c): None for c in punctuation}\n","                    ).strip()  # strip off punctuations\n","\n","            return addr\n","\n","        print(\"Cleaning addresses...\", end=\" \")\n","\n","        self.df[\"Address\"] = self.df[\"Address\"].apply(\n","            lambda x: clean_string(x)\n","        )\n","        self.df[\"Address\"] = self.df[\"Address\"].apply(\n","            lambda x: NULL_ADDR.sub('', x)\n","        )\n","\n","        # replace empty string values with NULL\n","        self.df[\"Zip Code\"] = self.df[\"Zip Code\"].replace('', float(\"nan\"))\n","        self.df[\"Address\"] = self.df[\"Address\"].replace('', float(\"nan\"))\n","\n","        print(\"Done\")\n","\n","    @staticmethod\n","    def combine_rows(row):\n","        \"\"\"Merges rows after filtering out common values\n","\n","        Args:\n","            row (`list` of `list` of `str`): groupby(\"Case Number\") rows\n","\n","        Returns:\n","            (`list` of `str`): merged row\n","        \"\"\"\n","\n","        def __filter_tuple(col):\n","            \"\"\"Filters common values from rows\n","\n","            Args:\n","                col (`tuple` of `str`): values per column\n","\n","            Returns:\n","                value (`str`): common value found per mergeable rows\n","            \"\"\"\n","\n","            for value in set(col):\n","                if value == value:  # equivalent to value != NaN\n","                    return value\n","\n","        return [__filter_tuple(x) for x in zip(*row)]\n","\n","    @staticmethod\n","    def mergeable(bool_vec):\n","        \"\"\"Determines if groupby(\"Case Number\") rows are mergeable\n","\n","        Example:\n","            bool_vec = [\n","                [True, True, True, True, True, True, False, True, True],\n","                [True, True, True, True, True, True, True, False, False],\n","                [True, True, True, True, True, True, False, False, False]\n","            ]\n","\n","            __sum_col(bool_vec) -> [3, 3, 3, 3, 3, 3, 1, 1, 1]\n","\n","            __bool_pat(__sum_col(bool_vec)) -> True\n","\n","        Args:\n","            bool_vec (`list` of `bool`): represents non-NULL values\n","\n","        Returns:\n","            (`bool`): True if rows are mergeable\n","        \"\"\"\n","\n","        def __sum_col():\n","            \"\"\"Sums columns\n","\n","            Args:\n","                None\n","\n","            Returns:\n","                (`list` of `int`): sum of columns\n","            \"\"\"\n","            return [sum(x) for x in zip(*bool_vec)]\n","\n","        def __bool_pat(row):\n","            \"\"\"Determines mergeability\n","\n","            Args:\n","                None\n","\n","            Returns:\n","                (`bool`): True if rows are mergeable\n","            \"\"\"\n","            return set(row[-3:]) == set([1]) and set(row[:-3]) != set([1])\n","\n","        return True if __bool_pat(__sum_col()) else False\n","\n","    def merge_nulls(self):\n","        \"\"\"Splits DataFrames into those with NULL values to be merged, and then\n","        later merged with the original DataFrame\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","\n","        print(\"Merging rows...\", end=\" \")\n","\n","        # filter out rows with any NULL values\n","        origin_df = self.df.dropna()\n","\n","        # filter out rows only with NULL values\n","        null_df = self.df[self.df.isnull().any(axis=1)]\n","\n","        # boolean representation of the DataFrame with NULL values\n","        bool_df = null_df.notnull()\n","\n","        # (`list` of `dict` of `str` : `str`) to be converted to a DataFrame\n","        new_df = []\n","\n","        for i in null_df[\"Case Number\"].unique():\n","            bool_row = bool_df[null_df[\"Case Number\"] == i]\n","            new_row = null_df[null_df[\"Case Number\"] == i]\n","\n","            # if the rows are mergeable, combine them\n","            if self.mergeable(bool_row.values):\n","                new_row = self.combine_rows(new_row.values.tolist())\n","\n","                new_df.append(\n","                    {\n","                        feature: value\n","                        for feature, value in zip(self.columns, new_row)\n","                    }\n","                )\n","\n","            # else, treat them individually\n","            else:\n","                new_row = new_row.values.tolist()\n","\n","                for row in new_row:\n","                    new_df.append(\n","                        {\n","                            feature: value\n","                            for feature, value in zip(self.columns, row)\n","                        }\n","                    )\n","\n","        # merge the DataFrames back\n","        self.clean_df = concat(\n","            [origin_df, DataFrame(new_df)]\n","        ).reset_index(drop=True)\n","\n","        # prettify the new DataFrame\n","        self.clean_df = self.prettify(\n","            self.clean_df[self.columns], internal=False\n","        )\n","\n","        print(\"Done\")\n","\n","    def init_clean(self):\n","        \"\"\"Initializes cleaning process\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","        \"\"\"\n","\n","        self.clean_addr()\n","        self.merge_nulls()\n","\n","    def download(self, output_name):\n","        \"\"\"Downloads the cleaned and manipulated DataFrame into a CSV file\n","\n","        Args:\n","            output_name (`str`): path of the new output file\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        self.clean_df.to_csv(output_name, index=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3vL8wy5lhDN","colab_type":"code","outputId":"f297dd02-439a-4fb7-833c-e6c84a3f4f14","executionInfo":{"status":"ok","timestamp":1574706487330,"user_tz":300,"elapsed":6079,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["temp_output = \"tem_dat.csv\"\n","output = 'outputfile.csv'\n","df_obj = Cleaner(temp_output)\n","df_obj.init_clean()\n","df_obj.download(output)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cleaning addresses... Done\n","Merging rows... Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t_qAlZURnaVL","colab_type":"text"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"DkIgIQk3ne50","colab_type":"code","colab":{}},"source":["\"\"\"main\n","\n","The main executable script for Close Crawl. This file manages types, flags\n","and constraints for the case type, year and output data file.\n","\n","Usage:\n","    $ python main.py <case_type> <case_year> <path/of/new/dataset>\n","      <opt: lower_bound> <opt: upper_bound> <opt: debug>\n","\n","Example usage:\n","    $ python main.py O 2015 test_set.csv -l=300 -u=600 -d=1\n","\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUsaGkBIncGp","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, print_function, unicode_literals\n","from json import dump, dumps, load\n","from os import path, remove, walk\n","from shutil import rmtree\n","from time import time\n","\n","from cleaner import Cleaner\n","from miner import Miner\n","from settings import CHECKPOINT, HTML_DIR\n","from spider import Spider"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4brxrwPnaBxq","colab_type":"code","colab":{}},"source":["def close_crawl(case_type, case_year, output, cases='', lower_bound=0,\n","                upper_bound=0, debug=False, scrape=True, mine=True,\n","                clean=True):\n","    \"\"\"Main function for Close Crawl.\n","\n","    Args:\n","        case_type (`str`): type of foreclosure case, options are 'O' and 'C'\n","        case_year (`str`): year of foreclosure cases\n","        output (`str`): path of the output CSV file, along with the valid\n","            extension (.csv)\n","        lower_bound (`int`, optional): lower bound of range of cases\n","        upper_bound (`int`, optional): upper bound of range of cases\n","        debug (`bool`, optional): option for switching between debug mode.\n","            Default -> True\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    temp_output = \"temp_data.csv\"\n","    wait = 0\n","    case_list = []\n","    \n","    print('checkpoint')\n","    if not path.isfile(CHECKPOINT):\n","        print(\"Initializing project...\")\n","        with open(CHECKPOINT, \"w\") as checkpoint:\n","            dump(\n","                {\n","                    \"last_case\": \"{:04d}\".format(int(str(lower_bound)[-4:])),\n","                    \"type\": case_type,\n","                    \"year\": case_year[-2:],\n","                    \"error_case\": '',\n","                },\n","                checkpoint\n","            )\n","\n","    print('cases')\n","    if not cases:\n","\n","        with open(CHECKPOINT) as checkpoint:\n","            prev_bound = int(load(checkpoint)[\"last_case\"])\n","            if not lower_bound:\n","                lower_bound = prev_bound\n","            upper_bound = upper_bound if int(upper_bound) > int(lower_bound) \\\n","                else str(lower_bound + 5)\n","\n","        case_list = range(int(lower_bound), int(upper_bound) + 1)\n","\n","    else:\n","\n","        with open(cases) as manual_cases:\n","            case_list = sorted(list(set(load(manual_cases))))\n","\n","    print('scrape')\n","    if scrape:\n","        spider = Spider(\n","            case_type=case_type, year=case_year[-2:],\n","            bounds=case_list, gui=False\n","        )\n","\n","        spider.save_response()\n","\n","        wait = spider.WAITING_TIME\n","\n","    file_array = [filenames for (dirpath, dirnames, filenames)\n","                  in walk(HTML_DIR)][0]\n","\n","    start_mine = time()\n","    print('mine')\n","    if mine:\n","        miner = Miner(file_array, temp_output)\n","        miner.scan_files()\n","        miner.export()\n","\n","\n","    print('clean')\n","    if clean:\n","        df_obj = Cleaner(temp_output)\n","\n","        df_obj.init_clean()\n","        df_obj.download(output)\n","\n","    print('save')\n","    with open(CHECKPOINT, \"r+\") as checkpoint:\n","        checkpoint_data = load(checkpoint)\n","        checkpoint_data[\"last_case\"] = sorted(file_array)[-1].split('.')[0][-4:]\n","        checkpoint.seek(0)\n","        checkpoint.write(dumps(checkpoint_data))\n","        checkpoint.truncate()\n","\n","\n","    print(\"Crawling runtime: {0:.2f} s\".format((end_crawl - start_crawl)))\n","    print(\n","        \"Downloading runtime: {0:.2f} s\".format(\n","            ((end_crawl - start_crawl) - wait))\n","    )\n","    print(\"Mining runtime: {0:.2f} s\".format((end_mine - start_mine)))\n","    print(\"Program runtime: {0:.2f} s\".format((end - start)))\n","    print(\"------------ SCRAPING COMPLETED ------------\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJYL13g1_icS","colab_type":"code","colab":{}},"source":["args = {'type': 'C', 'year': '2014', 'output': 'outputfile.csv', 'file': '', 'lower': '1000', 'upper': '2000', 'debug': '0', 'scrape': True, 'mine': True, 'clean': True}\n","close_crawl(\n","    case_type=args[\"type\"], case_year=args[\"year\"], output=args[\"output\"],\n","    cases=args[\"file\"], lower_bound=args[\"lower\"],\n","    upper_bound=args[\"upper\"], debug=args[\"debug\"],\n","    scrape=args[\"scrape\"], mine=args[\"mine\"],\n","    clean=args[\"clean\"]\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SXQJI6JlNRB","colab_type":"text"},"source":["## CLI"]},{"cell_type":"code","metadata":{"id":"tAB64vlFoVX5","colab_type":"code","colab":{}},"source":["ls"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPB7zlGzjydH","colab_type":"code","colab":{}},"source":["cd ../"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8_w3weqZyxL","colab_type":"code","colab":{}},"source":["! python cliargs.py -l=50 -u=3500 -d -s -m C 2016 output.csv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPN30DKPzv0X","colab_type":"code","outputId":"0232ca7a-4ea5-4c1e-a392-16b0402d1fb9","executionInfo":{"status":"ok","timestamp":1572543136813,"user_tz":240,"elapsed":1863,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["checkpoint.json  cli.py    \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  _version.py\n","cliargs.py       \u001b[01;34mmodules\u001b[0m/  \u001b[01;34mresponses\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gHghURgeRmv0","colab_type":"code","outputId":"dbe625bc-60fd-479e-99ee-b08e362123e3","executionInfo":{"status":"ok","timestamp":1572974201018,"user_tz":300,"elapsed":248,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["\"\"\"cliargs\n","\n","The main command line script for Close Crawl. This file manages types, flags\n","and constraints for the case type, year and output data file as well as the\n","processing options.\n","\n","Parameters:\n","  {O,C}        | Type of foreclosure cases\n","  year         | Year of foreclosure cases\n","  output       | Path of output file\n","\n","Optional parameters:\n","  -h, --help   | Show this help message and exit\n","  -v, --version| Show program's version number and exit\n","  -l, --lower  | Lower bound of range of cases\n","  -u, --upper  | Upper bound of range of cases\n","  -f, --file   | Path of JSON array of cases\n","  -d, --debug  | Debug mode\n","  -s, --scrape | Scrape only\n","  -m, --mine   | Mine only\n","  -c, --clean  | Clean only\n","\n","Usage:\n","    $ python cliarg.py [-h] [-v] [-l] [-u] [-f] [-d] [-s] [-m] [-c]\n","                  {O,C} year output\n","\n","Example usages:\n","    $ python cliarg.py -l=50 -u=3500 -d -s -m C 2016 output.csv\n","    $ python cliarg.py -c=\"cases_to_scrape.json\" -d O 2014 output01.csv\n","\"\"\""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cliargs\\n\\nThe main command line script for Close Crawl. This file manages types, flags\\nand constraints for the case type, year and output data file as well as the\\nprocessing options.\\n\\nParameters:\\n  {O,C}        | Type of foreclosure cases\\n  year         | Year of foreclosure cases\\n  output       | Path of output file\\n\\nOptional parameters:\\n  -h, --help   | Show this help message and exit\\n  -v, --version| Show program\\'s version number and exit\\n  -l, --lower  | Lower bound of range of cases\\n  -u, --upper  | Upper bound of range of cases\\n  -f, --file   | Path of JSON array of cases\\n  -d, --debug  | Debug mode\\n  -s, --scrape | Scrape only\\n  -m, --mine   | Mine only\\n  -c, --clean  | Clean only\\n\\nUsage:\\n    $ python cliarg.py [-h] [-v] [-l] [-u] [-f] [-d] [-s] [-m] [-c]\\n                  {O,C} year output\\n\\nExample usages:\\n    $ python cliarg.py -l=50 -u=3500 -d -s -m C 2016 output.csv\\n    $ python cliarg.py -c=\"cases_to_scrape.json\" -d O 2014 output01.csv\\n'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"fqM4kYpkofWY","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, print_function, unicode_literals\n","import sys\n","from textwrap import dedent\n","\n","from modules import main"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RADe3ugVnM7i","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","\n","if __name__ == \"__main__\":\n","\n","    args = {}\n","\n","    args[\"type\"] = input(\"Enter type of case (1 char: {C, O}): \")\n","    args[\"year\"] = input(\"Enter year of case (4 digit int): \")\n","    args[\"output\"] = input(\"Enter name of output file (CSV file path): \")\n","\n","    opt = int(input(\"Enter 0 for manual parameters or 1 for automatic: \"))\n","    if bool(opt):\n","        args[\"file\"] = input(\"Enter name of cases file (JSON file path): \")\n","        args[\"lower\"] = args[\"upper\"] = 0\n","\n","    else:\n","        args[\"file\"] = \"\"\n","        args[\"lower\"] = input(\"Enter lower bound of cases (1-4 digit int): \")\n","        args[\"upper\"] = input(\"Enter upper bound of cases (1-4 digit int): \")\n","\n","    args[\"debug\"] = input(\n","        \"Enter 0 for default mode, 1 for debug (1 digit int): \"\n","    )\n","\n","    print(\n","        dedent(\n","            \"\"\"Processing options:\\n\\n\"\"\"\n","            \"\"\"For the following options, enter 0 to disable or 1 to enable.\"\"\"\n","            \"\"\"\\nNOTE: The script will exit if all but the mining step is \"\"\"\n","            \"\"\"enabled - data cannot be cleaned without being mined first.\"\"\"\n","        )\n","    )\n","\n","    args[\"scrape\"] = bool(input(\"Scrape: {0, 1}: \"))\n","    args[\"mine\"] = bool(input(\"Mine: {0, 1}: \"))\n","    args[\"clean\"] = bool(input(\"Clean: {0, 1}: \"))\n","\n","    # exit script if all but the mining step is enabled\n","    if (args[\"scrape\"] and not(args[\"mine\"]) and args[\"clean\"]):\n","        sys.exit(\"\\nData cannot be cleaned without being mined first.\")\n","\n","    print (args)\n","    main.close_crawl(\n","        case_type=args[\"type\"], case_year=args[\"year\"], output=args[\"output\"],\n","        cases=args[\"file\"], lower_bound=args[\"lower\"],\n","        upper_bound=args[\"upper\"], debug=args[\"debug\"],\n","        scrape=args[\"scrape\"], mine=args[\"mine\"],\n","        clean=args[\"clean\"]\n","    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vOVybMiTZw_x","colab_type":"text"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"a5-boZ9ImREF","colab_type":"code","outputId":"b9db7665-0e32-418a-daf2-080e15c9348f","executionInfo":{"status":"ok","timestamp":1574366654503,"user_tz":300,"elapsed":1314,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":82}},"source":["ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" 2018_foreclosures.csv   \u001b[0m\u001b[01;34mmodules\u001b[0m/            \u001b[01;34m__pycache__\u001b[0m/       _version.py\n"," cliargs.py              no_case.json        \u001b[01;34mresponses\u001b[0m/\n"," cli.py                  outputfile.csv      temp_data.csv\n","'Close Crawl.ipynb'      outputfile.gsheet   temp_data.gsheet\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"itQzVZMnZw15","colab_type":"code","colab":{}},"source":["! python cliargs.py -h"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-6SSGTtuZxoP","colab":{}},"source":["! python cliargs.py -d -m -c C 2019 output.csv"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0T4ZUTo5gQkg","colab_type":"code","outputId":"7f8648f5-e4e0-4a78-8330-378e51340c45","executionInfo":{"status":"ok","timestamp":1574438619154,"user_tz":300,"elapsed":2018,"user":{"displayName":"Charles Karpati","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBHHSakx1TuL3scSYoN86KCwMX21rjaAKwTEJVnnQ=s64","userId":"06248540216665543849"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["! python cliargs.py -l=9500 -u=9999 -c -s -d -m O 2019 output.csv"],"execution_count":0,"outputs":[{"output_type":"stream","text":["python3: can't open file 'cliargs.py': [Errno 2] No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ULclUPijn-qT","colab_type":"code","colab":{}},"source":["ls"],"execution_count":0,"outputs":[]}]}